For some reason the training was super slow (30mins per epoch) because of which I couldn't take any of my experiments to 50 epochs. Still trying to debug why it'd be so slow.

I was seeing overfitting in early experiments (even after using basic transformation augmentation) so I added a random eraser.

UPDATE: I figured out that I wasn't using the GPU runtime causing super slow training. I used the model previously trained till 38 epochs and 85.2% validation accuracy and ran it with reduced learning rate of 10e-4. This gave me a max validation accuracy of 90.4%.

### Max test accuracy: 84.46 after 23 epochs
<pre>
Epoch 18/50
Learning rate:  0.001
782/782 [==============================] - 1568s 2s/step - loss: 0.8059 - acc: 0.8084 - val_loss: 0.8004 - val_acc: 0.8139

Epoch 00018: val_acc improved from 0.79500 to 0.81390, saving model to /content/saved_models/cifar10_ResNet18v1_model.018.h5
Epoch 19/50
Learning rate:  0.001
782/782 [==============================] - 1582s 2s/step - loss: 0.7980 - acc: 0.8104 - val_loss: 0.8330 - val_acc: 0.8132

Epoch 00019: val_acc did not improve from 0.81390
Epoch 20/50
Learning rate:  0.001
782/782 [==============================] - 1587s 2s/step - loss: 0.7849 - acc: 0.8162 - val_loss: 0.7267 - val_acc: 0.8410

Epoch 00020: val_acc improved from 0.81390 to 0.84100, saving model to /content/saved_models/cifar10_ResNet18v1_model.020.h5
Epoch 21/50
Learning rate:  0.001
782/782 [==============================] - 1584s 2s/step - loss: 0.7795 - acc: 0.8192 - val_loss: 0.7958 - val_acc: 0.8179

Epoch 00021: val_acc did not improve from 0.84100
Epoch 22/50
Learning rate:  0.001
782/782 [==============================] - 1505s 2s/step - loss: 0.7697 - acc: 0.8202 - val_loss: 0.8385 - val_acc: 0.8073

Epoch 00022: val_acc did not improve from 0.84100
Epoch 23/50
Learning rate:  0.001
782/782 [==============================] - 1506s 2s/step - loss: 0.7608 - acc: 0.8241 - val_loss: 0.7128 - val_acc: 0.8446

Epoch 00023: val_acc improved from 0.84100 to 0.84460, saving model to /content/saved_models/cifar10_ResNet18v1_model.023.h5
Epoch 24/50
Learning rate:  0.001
782/782 [==============================] - 1507s 2s/step - loss: 0.7557 - acc: 0.8265 - val_loss: 0.7924 - val_acc: 0.8175

Epoch 00024: val_acc did not improve from 0.84460
Epoch 25/50
Learning rate:  0.001
460/782 [================>.............] - ETA: 9:47 - loss: 0.7469 - acc: 0.8286

</pre>


### Updated after 50 epochs
<pre>
Total params: 2,147,554
Trainable params: 2,144,138
Non-trainable params: 3,416
__________________________________________________________________________________________________
ResNet18v1
Using real-time data augmentation.
Epoch 1/50
Learning rate:  0.0001
782/782 [==============================] - 123s 157ms/step - loss: 0.5882 - acc: 0.8856 - val_loss: 0.5848 - val_acc: 0.8915

Epoch 00001: val_acc improved from -inf to 0.89150, saving model to /content/saved_models/cifar10_ResNet18v1_model.001.h5
Epoch 2/50
Learning rate:  0.0001
782/782 [==============================] - 111s 142ms/step - loss: 0.5559 - acc: 0.8956 - val_loss: 0.5543 - val_acc: 0.8969

Epoch 00002: val_acc improved from 0.89150 to 0.89690, saving model to /content/saved_models/cifar10_ResNet18v1_model.002.h5
Epoch 3/50
Learning rate:  0.0001
782/782 [==============================] - 89s 114ms/step - loss: 0.5364 - acc: 0.9005 - val_loss: 0.5635 - val_acc: 0.8966

Epoch 00003: val_acc did not improve from 0.89690
Epoch 4/50
Learning rate:  0.0001
782/782 [==============================] - 89s 114ms/step - loss: 0.5211 - acc: 0.9049 - val_loss: 0.5480 - val_acc: 0.8994

Epoch 00004: val_acc improved from 0.89690 to 0.89940, saving model to /content/saved_models/cifar10_ResNet18v1_model.004.h5
Epoch 5/50
Learning rate:  0.0001
782/782 [==============================] - 87s 111ms/step - loss: 0.5081 - acc: 0.9068 - val_loss: 0.5327 - val_acc: 0.9007

Epoch 00005: val_acc improved from 0.89940 to 0.90070, saving model to /content/saved_models/cifar10_ResNet18v1_model.005.h5
Epoch 6/50
Learning rate:  0.0001
782/782 [==============================] - 86s 111ms/step - loss: 0.4973 - acc: 0.9094 - val_loss: 0.5337 - val_acc: 0.8991

Epoch 00006: val_acc did not improve from 0.90070
Epoch 7/50
Learning rate:  0.0001
782/782 [==============================] - 90s 115ms/step - loss: 0.4937 - acc: 0.9091 - val_loss: 0.5397 - val_acc: 0.8984

Epoch 00007: val_acc did not improve from 0.90070
Epoch 8/50
Learning rate:  0.0001
782/782 [==============================] - 88s 113ms/step - loss: 0.4831 - acc: 0.9119 - val_loss: 0.5134 - val_acc: 0.9026

Epoch 00008: val_acc improved from 0.90070 to 0.90260, saving model to /content/saved_models/cifar10_ResNet18v1_model.008.h5
Epoch 9/50
Learning rate:  0.0001
782/782 [==============================] - 91s 117ms/step - loss: 0.4762 - acc: 0.9122 - val_loss: 0.5142 - val_acc: 0.9020

Epoch 00009: val_acc did not improve from 0.90260
Epoch 10/50
Learning rate:  0.0001
782/782 [==============================] - 86s 111ms/step - loss: 0.4702 - acc: 0.9138 - val_loss: 0.5184 - val_acc: 0.9004

Epoch 00010: val_acc did not improve from 0.90260
Epoch 11/50
Learning rate:  0.0001
782/782 [==============================] - 89s 114ms/step - loss: 0.4644 - acc: 0.9168 - val_loss: 0.5107 - val_acc: 0.9043

</pre>


### Network
<pre>

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 28)   784         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 28)   112         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 28)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 28)   7084        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 28)   112         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 28)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 28)   7084        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 28)   112         conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 28)   0           activation_1[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 28)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 28)   7084        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 28)   112         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 28)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 28)   7084        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 28)   112         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 28)   0           activation_3[0][0]               
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 28)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 16, 16, 56)   14168       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 56)   224         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 16, 16, 56)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 56)   28280       activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 56)   1624        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 56)   224         conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 16, 16, 56)   0           conv2d_8[0][0]                   
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 16, 16, 56)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 56)   28280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 16, 16, 56)   224         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 56)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 56)   28280       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 56)   224         conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 16, 16, 56)   0           activation_7[0][0]               
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 56)   0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 8, 8, 112)    56560       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 8, 112)    448         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 8, 8, 112)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 8, 8, 112)    113008      activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 8, 8, 112)    6384        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 112)    448         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 8, 8, 112)    0           conv2d_13[0][0]                  
                                                                 batch_normalization_11[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 8, 8, 112)    0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 112)    113008      activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 8, 8, 112)    448         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 112)    0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 112)    113008      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 112)    448         conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 112)    0           activation_11[0][0]              
                                                                 batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 112)    0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 4, 4, 224)    226016      activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 4, 4, 224)    896         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 4, 4, 224)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 4, 4, 224)    451808      activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 4, 4, 224)    25312       activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 4, 4, 224)    896         conv2d_17[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 4, 4, 224)    0           conv2d_18[0][0]                  
                                                                 batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 4, 4, 224)    0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 4, 4, 224)    451808      activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 4, 4, 224)    896         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 4, 4, 224)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 4, 4, 224)    451808      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 4, 4, 224)    896         conv2d_20[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 4, 4, 224)    0           activation_15[0][0]              
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 4, 4, 224)    0           add_8[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 224)    0           activation_17[0][0]              
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 224)          0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           2250        flatten_1[0][0]                  
==================================================================================================
Total params: 2,147,554
Trainable params: 2,144,138
Non-trainable params: 3,416

</pre>
